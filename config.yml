model:
  default: "ollama/gemma2:latest"
  provider: "ollama"

api:
  base_url: "http://localhost:11434"

model_configs:
  gpt-3.5-turbo:
    max_tokens: 4096
    temperature: 0.7
  gpt-4:
    max_tokens: 8192
    temperature: 0.7
  claude-3-opus-20240229:
    provider: "anthropic"
    # api_base: "https://api.anthropic.com/v1/messages"
    max_tokens: 4096
    temperature: 0.7

# Add other non-sensitive configuration options as needed