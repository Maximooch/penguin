# Not exclusive to OpenAI
# Check LiteLLM documentation to see how to use other providers
# https://docs.litellm.ai/docs/providers

diagnostics:
  enabled: true  # Global switch for diagnostics
  max_context_tokens: 200000
  log_to_file: false
  log_path: "logs/penguin.log"  # Optional, only used if log_to_file is true


model:
  default: "gpt-4o"
  provider: "openai"
  use_assistants_api: false # Add this line to toggle Assistants API usage

api:
  base_url: "https://api.openai.com/v1"

model_configs:
  gpt-4o: 
    max_tokens: 4096
    temperature: 0.4
    supports_vision: true
  gpt-3.5-turbo:
    max_tokens: 4096
    temperature: 0.7
  gpt-4:
    max_tokens: 8192
    temperature: 0.7
  claude-3-opus-20240229:
    provider: "anthropic"
    # api_base: "https://api.anthropic.com/v1/messages"
    max_tokens: 4096
    temperature: 0.7

# Add other non-sensitive configuration options as needed

# Tools:
# Toggle whatever tools you want to use, config.py then makes a list sending to core.py to inform what tools are available.
# 3rd party tools need to have a info.yml (txt, json, whatever type of text file) linked to that particular tool. So it could be loaded as a mini prompt.


# https://api.openai.com/v1