# Kimi-CLI Architecture Analysis

**Project:** [Kimi-CLI](https://github.com/MoonBit-AI/kimi-cli)
**Analysis Date:** 2025-11-05
**Purpose:** Reference architecture study for Penguin CLI improvements

---

## Table of Contents

1. [Executive Summary](#1-executive-summary)
2. [Language & Technology Stack](#2-language--technology-stack)
3. [Architecture Overview](#3-architecture-overview)
4. [CLI/TUI Implementation](#4-clitui-implementation)
5. [Core Components](#5-core-components)
6. [Agent Architecture](#6-agent-architecture)
7. [Tool System](#7-tool-system)
8. [State Management](#8-state-management)
9. [Design Patterns](#9-design-patterns)
10. [Code Quality](#10-code-quality)
11. [Key Learnings for Penguin](#11-key-learnings-for-penguin)
12. [Python vs TypeScript CLI Decision](#12-python-vs-typescript-cli-decision)

---

## 1. Executive Summary

### The Critical Finding: 100% Pure Python

**Kimi-CLI is 100% Python - No Go, No Rust, No Other Languages**

This is the most important finding for Penguin: **A pure Python CLI is not just viable, it's proven and production-ready.**

### Key Characteristics

- **Scale:** ~8,546 lines of Python across 95 files
- **Language:** Python 3.13+ (100% pure Python)
- **Architecture:** Event-driven TUI with async/await throughout
- **LLM Integration:** Custom `kosong` framework (provider-agnostic)
- **UI Framework:** prompt-toolkit + rich (sophisticated terminal UI)
- **Agent System:** YAML-based agent specs with tool composition

### Maturity Level

**Production-ready** with:
- âœ… Sophisticated TUI with multiple modes
- âœ… Comprehensive tool system (15+ tools)
- âœ… Full async/await architecture
- âœ… Strong type safety (type hints + pyright)
- âœ… Testing infrastructure (pytest + async)
- âœ… Binary distribution (PyInstaller)
- âœ… MCP (Model Context Protocol) support
- âœ… Time-travel debugging
- âœ… Cross-platform (macOS, Linux, Windows coming)

---

## 2. Language & Technology Stack

### 2.1 Language Breakdown

```
Python:   8,546 lines (100%)
Go:       0 lines (0%)
Rust:     0 lines (0%)
Other:    0 lines (0%)
```

**Files:**
- Python modules: 95 files
- Test files: 28 files
- Total: 123 .py files

### 2.2 Core Dependencies

**CLI/TUI Framework:**
```python
click==8.3.0                # CLI framework (commands, args)
prompt-toolkit==3.0.52      # Advanced TUI (readline replacement)
rich==14.2.0                # Terminal formatting & colors
```

**LLM Integration:**
```python
kosong==0.17.0              # Custom LLM abstraction framework
pydantic==2.12.3            # Data validation & settings
aiohttp==3.13.2             # Async HTTP client
```

**File Operations:**
```python
aiofiles==25.1.0            # Async file I/O
watchfiles==1.0.3           # File watching
```

**Tool-Specific:**
```python
ripgrepy==2.2.0             # Python wrapper for ripgrep
patch-ng==1.19.0            # Patch file support
trafilatura==2.0.0          # Web content extraction
pillow==12.0.0              # Image handling (clipboard paste)
fastmcp==2.12.5             # MCP (Model Context Protocol)
```

**Quality Assurance:**
```python
pytest==8.4.0               # Testing framework
pytest-asyncio==1.0.0       # Async test support
pyright==1.2.0              # Type checking (strict mode)
ruff==0.11.1                # Linting & formatting
tenacity==10.0.0            # Retry logic with exponential backoff
```

**Distribution:**
```python
pyinstaller==6.14.0         # Standalone binary creation
```

### 2.3 Why No Go/Rust?

**Performance:** Python's async I/O is sufficient for:
- Network operations (LLM API calls)
- File operations (reading/writing)
- Terminal rendering (TUI updates)

**Binary Tools:** External tools (like ripgrep) are:
- Downloaded at runtime if needed
- Bundled in binary distribution
- Called via subprocess

**No Performance Bottlenecks:**
- LLM API calls are network-bound (Python overhead negligible)
- Terminal rendering is fast enough with rich/prompt-toolkit
- File operations are async (no blocking)

---

## 3. Architecture Overview

### 3.1 High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Kimi-CLI (Python)                       â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              UI Layer                               â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚    â”‚
â”‚  â”‚  â”‚  Shell   â”‚  â”‚   ACP    â”‚  â”‚   Wire   â”‚         â”‚    â”‚
â”‚  â”‚  â”‚  (TUI)   â”‚  â”‚ Server   â”‚  â”‚ Protocol â”‚         â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚          â”‚             â”‚             â”‚                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚           Agent Layer (KimiSoul)                    â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â”‚  - Agentic loop                              â”‚  â”‚    â”‚
â”‚  â”‚  â”‚  - Context management                        â”‚  â”‚    â”‚
â”‚  â”‚  â”‚  - Checkpointing                             â”‚  â”‚    â”‚
â”‚  â”‚  â”‚  - Tool execution                            â”‚  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚            â”‚                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         LLM Layer (Kosong)                          â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚    â”‚
â”‚  â”‚  â”‚   Kimi   â”‚  â”‚  OpenAI  â”‚  â”‚Anthropic â”‚         â”‚    â”‚
â”‚  â”‚  â”‚   API    â”‚  â”‚   API    â”‚  â”‚   API    â”‚         â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚            â”‚                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Tool Layer                                  â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”    â”‚    â”‚
â”‚  â”‚  â”‚ Bash â”‚ â”‚ File â”‚ â”‚ Grep â”‚ â”‚ Web  â”‚ â”‚ MCP  â”‚    â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚            â”‚                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Storage Layer                               â”‚    â”‚
â”‚  â”‚  - JSONL conversation history                       â”‚    â”‚
â”‚  â”‚  - JSON configuration                               â”‚    â”‚
â”‚  â”‚  - Session metadata                                 â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 Multi-Mode Design

**Three UI Modes:**

1. **Shell Mode (Default)** - Interactive TUI
   ```bash
   kimi  # Launches interactive shell
   ```
   - Rich terminal UI with prompt-toolkit
   - Dual-mode: Agent mode â†” Shell mode
   - Streaming responses with live updates
   - File completion with @mentions
   - Image paste support (Ctrl-V)

2. **Print Mode** - Non-interactive (for scripts/automation)
   ```bash
   kimi --mode print "Write hello world"
   ```
   - No TUI, just plain output
   - Good for scripting, CI/CD
   - Still supports streaming

3. **ACP Server** - Agent Client Protocol (for IDE integration)
   ```bash
   kimi --mode acp
   ```
   - JSON-RPC protocol
   - Used by Zed editor integration
   - Headless agent execution

4. **Wire Protocol** - Internal event streaming
   ```bash
   kimi --mode wire
   ```
   - Low-level event protocol
   - For custom integrations
   - Debugging tool

### 3.3 Directory Structure

```
kimi_cli/
â”œâ”€â”€ __init__.py                 # Package init
â”œâ”€â”€ __main__.py                 # Entry point (python -m kimi_cli)
â”œâ”€â”€ cli.py                      # CLI commands (click)
â”‚
â”œâ”€â”€ config.py                   # Configuration management (285 LOC)
â”œâ”€â”€ llm.py                      # LLM provider factory (86 LOC)
â”œâ”€â”€ metadata.py                 # Session metadata (85 LOC)
â”œâ”€â”€ session.py                  # Session management (117 LOC)
â”‚
â”œâ”€â”€ soul/                       # Agent layer
â”‚   â”œâ”€â”€ context.py             # Conversation history (JSONL) (155 LOC)
â”‚   â”œâ”€â”€ kimisoul.py            # Main agent loop (319 LOC)
â”‚   â”œâ”€â”€ loader.py              # Agent spec loading (169 LOC)
â”‚   â”œâ”€â”€ approval.py            # User approval flow (68 LOC)
â”‚   â””â”€â”€ denwarenji.py          # Time-travel debugging (14 LOC)
â”‚
â”œâ”€â”€ tools/                      # Tool implementations
â”‚   â”œâ”€â”€ bash.py                # Shell execution (154 LOC)
â”‚   â”œâ”€â”€ think.py               # Internal reasoning (18 LOC)
â”‚   â”œâ”€â”€ dmail/                 # Time-travel messaging (33 LOC)
â”‚   â”œâ”€â”€ file/                  # File operations
â”‚   â”‚   â”œâ”€â”€ read.py           # Read files (109 LOC)
â”‚   â”‚   â”œâ”€â”€ write.py          # Write files (67 LOC)
â”‚   â”‚   â”œâ”€â”€ glob.py           # Pattern matching (86 LOC)
â”‚   â”‚   â”œâ”€â”€ grep.py           # Content search (287 LOC)
â”‚   â”‚   â”œâ”€â”€ str_replace.py    # String replacement (105 LOC)
â”‚   â”‚   â””â”€â”€ patch.py          # Patch files (106 LOC)
â”‚   â”œâ”€â”€ web/                   # Web tools
â”‚   â”‚   â”œâ”€â”€ search.py         # Web search (78 LOC)
â”‚   â”‚   â””â”€â”€ fetch.py          # URL fetching (115 LOC)
â”‚   â”œâ”€â”€ task.py                # Subagent delegation (87 LOC)
â”‚   â””â”€â”€ todo.py                # Task management (58 LOC)
â”‚
â”œâ”€â”€ ui/                         # UI implementations
â”‚   â”œâ”€â”€ shell/                 # Interactive TUI
â”‚   â”‚   â”œâ”€â”€ __init__.py       # Main shell app (303 LOC)
â”‚   â”‚   â”œâ”€â”€ prompt.py         # Input handling (789 LOC) â­ LARGEST
â”‚   â”‚   â”œâ”€â”€ liveview.py       # Live display (417 LOC)
â”‚   â”‚   â””â”€â”€ visualize.py      # Message rendering (224 LOC)
â”‚   â”œâ”€â”€ acp/                   # ACP server
â”‚   â”‚   â””â”€â”€ __init__.py       # JSON-RPC server (436 LOC)
â”‚   â””â”€â”€ wire/                  # Wire protocol
â”‚       â”œâ”€â”€ __init__.py       # Event server (340 LOC)
â”‚       â””â”€â”€ message.py        # Event types (81 LOC)
â”‚
â”œâ”€â”€ utils/                      # Utilities
â”‚   â”œâ”€â”€ terminal.py            # Terminal helpers (45 LOC)
â”‚   â”œâ”€â”€ gitignore.py          # .gitignore parsing (127 LOC)
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ agents/                     # Built-in agent specs
    â””â”€â”€ default/               # Default agent
        â”œâ”€â”€ agent.yaml        # Agent configuration
        â””â”€â”€ system.md         # System prompt
```

---

## 4. CLI/TUI Implementation

### 4.1 CLI Framework (Click)

**Entry Point:**

```python
# cli.py
import click

@click.command()
@click.option("--mode", type=click.Choice(["shell", "print", "acp", "wire"]))
@click.option("--continue/--new", default=True)
@click.argument("prompt", required=False)
async def main(mode: str, continue_: bool, prompt: str | None):
    """Kimi CLI - AI coding assistant"""

    if mode == "shell":
        await shell_mode(continue_)
    elif mode == "print":
        await print_mode(prompt)
    elif mode == "acp":
        await acp_server()
    elif mode == "wire":
        await wire_server()
```

### 4.2 TUI Implementation (prompt-toolkit + rich)

**Input Handling** (`ui/shell/prompt.py` - 789 lines, LARGEST FILE):

```python
from prompt_toolkit import PromptSession
from prompt_toolkit.key_binding import KeyBindings

class CustomPromptSession(PromptSession):
    """Custom prompt with rich features"""

    def __init__(self):
        # Key bindings
        kb = KeyBindings()

        @kb.add("c-x")  # Ctrl-X: Switch agent/shell mode
        def _(event):
            self.toggle_mode()

        @kb.add("c-j")  # Ctrl-J: New line
        def _(event):
            event.current_buffer.insert_text("\n")

        @kb.add("c-v")  # Ctrl-V: Paste image
        async def _(event):
            image = await clipboard.get_image()
            if image:
                self.insert_image_placeholder(image)

        super().__init__(
            multiline=True,
            key_bindings=kb,
            completer=self.create_completer(),
            bottom_toolbar=self.create_toolbar(),
        )

    def create_completer(self):
        """Autocomplete for @mentions and /commands"""
        return merge_completers([
            MetaCommandCompleter(),      # /help, /setup, /history
            FileMentionCompleter(),      # @src/main.py
        ])
```

**File Mention Completion:**

```python
class FileMentionCompleter(Completer):
    """Fuzzy file path completion with @ prefix"""

    def get_completions(self, document, complete_event):
        # User types: "@src/ma"
        text = document.text

        if not text.startswith("@"):
            return

        query = text[1:]  # "src/ma"

        # Fuzzy match files (respects .gitignore)
        files = self.fuzzy_search(query)

        for file in files[:10]:  # Top 10 matches
            yield Completion(
                file.path,
                start_position=-len(query),
                display=f"@ {file.path}",
                display_meta=f"{file.size} bytes"
            )
```

**Image Paste Support:**

```python
async def handle_paste():
    """Paste image from clipboard (Ctrl-V)"""

    # Get image from clipboard (macOS: pngpaste, Linux: xclip)
    image = await clipboard.get_image()

    if not image:
        return

    # Save to temp file
    path = save_temp_image(image)

    # Insert placeholder in prompt
    placeholder = f"[image:{path.name},{image.width}x{image.height}]"
    buffer.insert_text(placeholder)

    # Will be sent as ImageURLPart in message
```

**Bottom Toolbar:**

```python
def create_toolbar():
    """Status bar at bottom of terminal"""

    def get_toolbar_text():
        mode = "Agent" if agent_mode else "Shell"
        context_usage = f"{context.tokens_used}/{context.max_tokens}"
        time_str = datetime.now().strftime("%H:%M:%S")

        return f" {mode} | Context: {context_usage} | {time_str} "

    return get_toolbar_text
```

**Output Rendering** (`ui/shell/liveview.py` - 417 lines):

```python
from rich.console import Console
from rich.live import Live
from rich.markdown import Markdown
from rich.syntax import Syntax

class LiveView:
    """Live-updating display for streaming responses"""

    def __init__(self):
        self.console = Console()
        self.live = Live(console=self.console, refresh_per_second=4)

    async def stream_response(self, agent_iterator):
        """Display streaming response with live updates"""

        with self.live:
            async for event in agent_iterator:
                if isinstance(event, MessagePart):
                    # Append text to display
                    self.append_text(event.text)

                elif isinstance(event, ToolCallBegin):
                    # Show tool execution header
                    self.show_tool_header(event.tool_name)

                elif isinstance(event, ToolCallOutput):
                    # Show tool output (collapsible)
                    self.show_tool_output(event.output)

                elif isinstance(event, StatusUpdate):
                    # Update status bar
                    self.update_status(event.status)

                # Refresh display
                self.live.update(self.render())

    def render(self):
        """Render current state to Rich renderables"""

        layout = Layout()

        # Message area (with markdown)
        layout.add_panel(
            Markdown(self.current_text),
            title="Assistant",
            border_style="blue"
        )

        # Tool executions (collapsible)
        for tool in self.active_tools:
            layout.add_panel(
                self.render_tool(tool),
                title=f"ðŸ”§ {tool.name}",
                border_style="yellow"
            )

        # Status bar
        layout.add_footer(self.status_bar)

        return layout
```

**Message Visualization** (`ui/shell/visualize.py`):

```python
def visualize_message(message: Message) -> RenderableType:
    """Render message with rich formatting"""

    parts = []

    for part in message.content:
        if isinstance(part, TextPart):
            # Markdown text
            parts.append(Markdown(part.text))

        elif isinstance(part, ToolCallPart):
            # Tool execution
            parts.append(render_tool_call(part))

        elif isinstance(part, ImageURLPart):
            # Image (show dimensions, not actual image)
            parts.append(
                Text(f"ðŸ–¼ï¸  Image: {part.url} ({part.width}x{part.height})")
            )

    return Group(*parts)

def render_tool_call(tool: ToolCallPart) -> Panel:
    """Render tool execution as panel"""

    # Input (collapsed by default)
    input_tree = Tree("Input")
    for key, value in tool.input.items():
        input_tree.add(f"{key}: {value}")

    # Output (syntax highlighted)
    output = Syntax(tool.output, "text", theme="monokai")

    # Status badge
    status = Text(tool.status, style="bold green" if tool.status == "success" else "bold red")

    return Panel(
        Group(input_tree, output),
        title=f"ðŸ”§ {tool.name} - {status}",
        border_style="yellow"
    )
```

---

## 5. Core Components

### 5.1 LLM Integration (`llm.py`, `kosong` framework)

**Provider Factory:**

```python
from kosong import Kimi, OpenAI, Anthropic, OpenAIResponses

def create_llm(
    provider: LLMProvider,
    model: LLMModel,
    *,
    stream: bool = True,
    session_id: str | None = None,
) -> LLM:
    """Create LLM client from configuration"""

    match provider.type:
        case "kimi":
            return Kimi(
                model=model.model,
                base_url=provider.base_url,
                api_key=provider.api_key.get_secret_value(),
                stream=stream,
                default_headers={"User-Agent": USER_AGENT},
            )

        case "openai":
            return OpenAI(
                model=model.model,
                api_key=provider.api_key.get_secret_value(),
                stream=stream,
            )

        case "openai_responses":
            # OpenAI Responses API (with Pause/Resume)
            return OpenAIResponses(
                model=model.model,
                api_key=provider.api_key.get_secret_value(),
                session_id=session_id,
            )

        case "anthropic":
            return Anthropic(
                model=model.model,
                api_key=provider.api_key.get_secret_value(),
                stream=stream,
            )

        case _:
            raise ValueError(f"Unknown provider type: {provider.type}")
```

**Kosong Framework** (Custom LLM Abstraction):

```python
# Simplified kosong API
import kosong

# Step execution
result = await kosong.step(
    chat_provider,           # LLM client
    system_prompt,           # System instructions
    toolset,                 # Available tools
    history,                 # Conversation history
    on_message_part=callback,    # Streaming text callback
    on_tool_result=callback,     # Tool result callback
)

# Wait for tool results
tool_results = await result.tool_results()

# Grow context with new messages
await context.grow(result, tool_results)
```

**Streaming Handler:**

```python
async def on_message_part(part: MessagePart):
    """Called for each streaming token"""
    wire_send(MessagePart(text=part.text))
    live_view.append_text(part.text)

async def on_tool_result(result: ToolResult):
    """Called when tool execution completes"""
    wire_send(ToolCallOutput(output=result.output))
    live_view.show_tool_result(result)
```

### 5.2 Context Management (`soul/context.py`)

**JSONL Storage Format:**

```python
class Context:
    """Conversation history stored as JSONL"""

    def __init__(self, history_file: Path):
        self._history_file = history_file
        self._history: list[Message] = []
        self._token_count = 0
        self._next_checkpoint_id = 0

    async def load(self):
        """Load conversation history from JSONL file"""

        if not self._history_file.exists():
            return

        async with aiofiles.open(self._history_file) as f:
            async for line in f:
                obj = json.loads(line)

                if obj["role"] == "_checkpoint":
                    # Checkpoint marker
                    self._next_checkpoint_id = obj["id"] + 1

                elif obj["role"] == "_usage":
                    # Token count tracking
                    self._token_count = obj["token_count"]

                else:
                    # Regular message
                    self._history.append(Message.parse_obj(obj))

    async def append_message(self, message: Message):
        """Append message to history (async file I/O)"""

        # Add to in-memory list
        self._history.append(message)

        # Append to JSONL file (one line per message)
        async with aiofiles.open(self._history_file, "a") as f:
            await f.write(json.dumps(message.dict()) + "\n")

    async def checkpoint(self, add_user_message: bool = True):
        """Create checkpoint for time-travel"""

        checkpoint_id = self._next_checkpoint_id
        self._next_checkpoint_id += 1

        # Write checkpoint marker
        async with aiofiles.open(self._history_file, "a") as f:
            await f.write(json.dumps({
                "role": "_checkpoint",
                "id": checkpoint_id,
                "timestamp": time.time()
            }) + "\n")

        if add_user_message:
            # Checkpoint with a user message placeholder
            await self.append_message(Message(
                role="user",
                content=[TextPart(text="[checkpoint]")]
            ))

    async def revert_to(self, checkpoint_id: int):
        """Time-travel to previous checkpoint"""

        # Read entire history
        lines = []
        async with aiofiles.open(self._history_file) as f:
            async for line in f:
                lines.append(line)

        # Find checkpoint line
        checkpoint_line_idx = None
        for i, line in enumerate(lines):
            obj = json.loads(line)
            if obj.get("role") == "_checkpoint" and obj.get("id") == checkpoint_id:
                checkpoint_line_idx = i
                break

        if checkpoint_line_idx is None:
            raise ValueError(f"Checkpoint {checkpoint_id} not found")

        # Rotate old history (backup)
        backup_file = self._history_file.with_suffix(".jsonl.bak")
        shutil.copy(self._history_file, backup_file)

        # Write truncated history
        async with aiofiles.open(self._history_file, "w") as f:
            for line in lines[:checkpoint_line_idx + 1]:
                await f.write(line)

        # Reload history
        self._history.clear()
        await self.load()
```

**Context Compaction:**

```python
async def compact_context(self):
    """Summarize history when context too long"""

    # 1. Generate summary using LLM
    summary_prompt = """
    Summarize the conversation so far.
    Focus on:
    - User's goals
    - Key decisions made
    - Current state of the task
    """

    summary_result = await kosong.step(
        chat_provider,
        summary_prompt,
        toolset=[],  # No tools
        history=self._history,
    )

    summary_text = summary_result.message.content[0].text

    # 2. Revert to checkpoint 0
    await self.revert_to(0)

    # 3. Replace history with summary
    await self.append_message(Message(
        role="user",
        content=[TextPart(text=summary_text)]
    ))

    # 4. Continue with more room
    self._token_count = count_tokens(summary_text)
```

### 5.3 Session Management (`session.py`, `metadata.py`)

**Session Model:**

```python
@dataclass
class Session:
    """Per-directory session"""
    id: str                    # UUID
    work_dir: Path            # Working directory
    history_file: Path        # JSONL conversation history

    @staticmethod
    def create(work_dir: Path) -> Session:
        """Create new session (truncate if exists)"""

        session_id = str(uuid.uuid4())

        # Session directory
        sessions_dir = get_sessions_dir(work_dir)
        sessions_dir.mkdir(parents=True, exist_ok=True)

        # History file
        history_file = sessions_dir / f"{session_id}.jsonl"
        history_file.touch()  # Create empty file

        # Update metadata
        update_metadata(work_dir, session_id)

        return Session(
            id=session_id,
            work_dir=work_dir,
            history_file=history_file
        )

    @staticmethod
    def continue_(work_dir: Path) -> Session | None:
        """Resume last session for directory"""

        meta = load_metadata(work_dir)
        if not meta or not meta.last_session_id:
            return None

        session_id = meta.last_session_id
        history_file = get_sessions_dir(work_dir) / f"{session_id}.jsonl"

        if not history_file.exists():
            return None

        return Session(
            id=session_id,
            work_dir=work_dir,
            history_file=history_file
        )
```

**Metadata Storage:**

```python
# ~/.kimi/metadata.json
class WorkDirMeta(BaseModel):
    """Metadata for a working directory"""
    path: str
    last_session_id: str | None
    sessions_dir: Path

class Metadata(BaseModel):
    """Global metadata"""
    work_dirs: dict[str, WorkDirMeta]

def update_metadata(work_dir: Path, session_id: str):
    """Update metadata after session creation/continuation"""

    meta = load_metadata()

    work_dir_key = str(work_dir.resolve())

    if work_dir_key not in meta.work_dirs:
        meta.work_dirs[work_dir_key] = WorkDirMeta(
            path=work_dir_key,
            last_session_id=session_id,
            sessions_dir=get_sessions_dir(work_dir)
        )
    else:
        meta.work_dirs[work_dir_key].last_session_id = session_id

    # Write to ~/.kimi/metadata.json
    save_metadata(meta)
```

---

## 6. Agent Architecture

### 6.1 KimiSoul (Main Agent Loop)

**File:** `soul/kimisoul.py` (319 lines)

```python
class KimiSoul(Soul):
    """Main agent execution engine"""

    def __init__(
        self,
        chat_provider: LLM,
        toolset: ToolSet,
        context: Context,
        runtime: Runtime,
        config: Config,
    ):
        self.chat_provider = chat_provider
        self.toolset = toolset
        self.context = context
        self.runtime = runtime
        self.config = config

    async def run(self, wire_send: Callable):
        """Execute agent loop"""

        # Checkpoint before starting
        await self._checkpoint()

        # Main loop
        await self._agent_loop(wire_send)

    async def _agent_loop(self, wire_send: Callable):
        """The main agent loop"""

        step_no = 1
        max_steps = self.config.loop_control.max_agent_steps

        while step_no <= max_steps:
            # Event: Step begin
            wire_send(StepBegin(step_no=step_no))

            # 1. Check context length
            if self.context.token_count > self.context.max_tokens * 0.9:
                # Context too long - compact it
                await self.compact_context()

            # 2. Checkpoint state (for rollback)
            await self._checkpoint()

            # 3. Run LLM step with retry logic
            result = await self._kosong_step_with_retry(wire_send)

            # 4. Wait for tool results
            tool_results = await result.tool_results()

            # 5. Grow context with new messages
            await self._grow_context(result, tool_results)

            # 6. Check if finished
            if result.finish_reason == "stop":
                # Agent decided to finish
                wire_send(RunFinish(reason="stop"))
                break

            # 7. Continue loop
            step_no += 1

        if step_no > max_steps:
            wire_send(RunFinish(reason="max_steps"))

    @tenacity.retry(
        retry=retry_if_exception(lambda e: isinstance(e, RetryableError)),
        wait=wait_exponential_jitter(initial=0.3, max=5, jitter=0.5),
        stop=stop_after_attempt(10),
        reraise=True,
    )
    async def _kosong_step_with_retry(self, wire_send: Callable) -> StepResult:
        """Execute one LLM step with retry logic"""

        return await kosong.step(
            self.chat_provider,
            system_prompt=self.get_system_prompt(),
            toolset=self.toolset,
            history=self.context.history,
            on_message_part=lambda part: wire_send(MessagePart(text=part.text)),
            on_tool_result=lambda result: wire_send(ToolCallOutput(
                tool_name=result.tool_name,
                output=result.output
            )),
        )

    async def _grow_context(self, result: StepResult, tool_results: list[ToolResult]):
        """Add new messages to context"""

        # Assistant message
        await self.context.append_message(result.message)

        # Tool results as user message
        if tool_results:
            await self.context.append_message(Message(
                role="user",
                content=[ToolResultPart(results=tool_results)]
            ))

    async def _checkpoint(self):
        """Create checkpoint for time-travel"""
        await self.context.checkpoint()
```

### 6.2 Agent Specification (YAML)

**Agent Spec Format:**

```yaml
# agents/default/agent.yaml
version: 1

agent:
  name: "Kimi Default Agent"
  system_prompt_path: "./system.md"

  tools:
    # Built-in tools
    - kimi_cli.tools.bash:Bash
    - kimi_cli.tools.think:Think
    - kimi_cli.tools.file.read:ReadFile
    - kimi_cli.tools.file.write:WriteFile
    - kimi_cli.tools.file.glob:Glob
    - kimi_cli.tools.file.grep:Grep
    - kimi_cli.tools.file.str_replace:StrReplaceFile
    - kimi_cli.tools.file.patch:PatchFile
    - kimi_cli.tools.web.search:SearchWeb
    - kimi_cli.tools.web.fetch:FetchURL
    - kimi_cli.tools.task:Task
    - kimi_cli.tools.dmail.send:SendDMail
    - kimi_cli.tools.todo:SetTodoList

  exclude_tools: []

  # Subagents
  subagents:
    research:
      path: "./agents/research/agent.yaml"
      description: "Research information online or in documentation"

    planner:
      path: "./agents/planner/agent.yaml"
      description: "Create detailed plans for complex tasks"
```

**Agent Loading:**

```python
# soul/loader.py
async def load_agent(spec_path: Path) -> KimiSoul:
    """Load agent from YAML spec"""

    # 1. Parse YAML
    with open(spec_path) as f:
        spec = yaml.safe_load(f)

    # 2. Load system prompt
    system_prompt_path = spec_path.parent / spec["agent"]["system_prompt_path"]
    system_prompt = system_prompt_path.read_text()

    # 3. Load tools
    toolset = ToolSet()
    for tool_path in spec["agent"]["tools"]:
        module_path, class_name = tool_path.rsplit(":", 1)
        module = importlib.import_module(module_path)
        tool_class = getattr(module, class_name)

        # Instantiate with dependency injection
        tool = tool_class(**tool_deps)
        toolset.add(tool)

    # 4. Load subagents
    subagents = {}
    for name, subspec in spec["agent"]["subagents"].items():
        subagent_path = spec_path.parent / subspec["path"]
        subagents[name] = await load_agent(subagent_path)

    # 5. Create agent
    return KimiSoul(
        chat_provider=chat_provider,
        toolset=toolset,
        context=context,
        runtime=runtime,
        config=config,
        subagents=subagents,
    )
```

### 6.3 Approval Flow

**User Approval for Dangerous Operations:**

```python
# soul/approval.py
class Approval:
    """Manages user approval for tool executions"""

    async def request(
        self,
        tool_name: str,
        brief: str,
        full: str,
    ) -> bool:
        """Request user approval"""

        # Check auto-approval rules
        if self.is_auto_approved(tool_name, brief):
            return True

        # Show approval prompt
        console.print(Panel(
            f"[yellow]{tool_name}[/yellow]\n\n{full}",
            title="Approval Required"
        ))

        # Get user input
        response = await prompt_toolkit.prompt(
            "Approve? [y/n/always/never]: "
        )

        if response == "always":
            # Add to auto-approval list
            self.add_auto_approval_rule(tool_name, brief)
            return True

        elif response == "never":
            # Add to never-approve list
            self.add_never_approve_rule(tool_name, brief)
            return False

        return response.lower() == "y"

    def is_auto_approved(self, tool_name: str, brief: str) -> bool:
        """Check if operation is auto-approved"""

        # Check config rules
        for rule in self.config.auto_approval_rules:
            if fnmatch(tool_name, rule.tool_pattern):
                if fnmatch(brief, rule.brief_pattern):
                    return True

        return False
```

**Auto-Approval Configuration:**

```yaml
# config.json
{
  "auto_approval_rules": [
    {
      "tool_pattern": "ReadFile",
      "brief_pattern": "*"
    },
    {
      "tool_pattern": "Bash",
      "brief_pattern": "ls *"
    },
    {
      "tool_pattern": "Bash",
      "brief_pattern": "cat *"
    }
  ]
}
```

### 6.4 Time-Travel Debugging (D-Mail)

**Inspired by Steins;Gate:**

```python
# tools/dmail/send.py
class SendDMail(CallableTool2[Params]):
    """Send message to past checkpoint (time-travel)"""

    name = "SendDMail"
    description = load_desc(Path(__file__).parent / "send.md")

    class Params(BaseModel):
        checkpoint_id: int
        message: str

    async def __call__(self, params: Params) -> ToolReturnType:
        """Send D-Mail (revert + add message)"""

        # 1. Revert to checkpoint
        await self.context.revert_to(params.checkpoint_id)

        # 2. Add new user message
        await self.context.append_message(Message(
            role="user",
            content=[TextPart(text=params.message)]
        ))

        return ToolOk(
            output="D-Mail sent! Timeline altered.",
            message=f"Reverted to checkpoint {params.checkpoint_id} with new message"
        )
```

**Usage:**

```
Agent: I made a mistake in step 3. Let me send a D-Mail.

[Agent calls SendDMail with checkpoint_id=2, message="Please use a different approach"]

[Context reverts to checkpoint 2]
[New user message added: "Please use a different approach"]
[Agent continues from checkpoint 2 with new guidance]
```

---

## 7. Tool System

### 7.1 Tool Architecture

**Base Pattern (using Kosong framework):**

```python
from kosong import CallableTool2
from pydantic import BaseModel

class MyTool(CallableTool2[Params]):
    """Tool docstring (shown to LLM)"""

    name: str = "MyTool"
    description: str = load_desc(Path(__file__).parent / "tool.md")
    params: type[Params] = Params

    class Params(BaseModel):
        """Parameter schema"""
        arg1: str
        arg2: int = 0

    def __init__(self, approval: Approval, runtime: Runtime, **kwargs):
        """Dependency injection"""
        super().__init__(**kwargs)
        self._approval = approval
        self._runtime = runtime

    async def __call__(self, params: Params) -> ToolReturnType:
        """Execute tool"""

        # 1. Request approval if needed
        if not await self._approval.request(
            self.name,
            brief=f"arg1={params.arg1}",
            full=f"Execute {self.name} with arg1={params.arg1}, arg2={params.arg2}"
        ):
            return ToolRejectedError()

        # 2. Execute logic
        try:
            result = await self.do_work(params)
            return ToolOk(output=result, message="Success")
        except Exception as e:
            return ToolError(message=str(e), brief="Failed")

    async def do_work(self, params: Params) -> str:
        """Actual tool logic"""
        ...
```

**Tool Description (Markdown):**

```markdown
<!-- tools/bash/tool.md -->

Execute a bash command in the working directory.

The command output (stdout and stderr) will be returned.

Use this for:
- Running build commands
- Executing tests
- Git operations
- File system operations

Example:
```bash
ls -la src/
```

Note: Interactive commands (like vim, nano) will not work.
```

### 7.2 Built-in Tools

**Bash Tool:**

```python
# tools/bash.py (154 lines)
class Bash(CallableTool2[Params]):
    """Execute shell commands"""

    class Params(BaseModel):
        command: str
        timeout: int = 120  # seconds

    async def __call__(self, params: Params) -> ToolReturnType:
        # Request approval
        if not await self._approval.request(
            "Bash",
            brief=params.command[:100],
            full=f"Execute: {params.command}"
        ):
            return ToolRejectedError()

        # Execute with timeout
        proc = await asyncio.create_subprocess_shell(
            params.command,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
            cwd=self._runtime.work_dir,
        )

        try:
            stdout, stderr = await asyncio.wait_for(
                proc.communicate(),
                timeout=params.timeout
            )
        except asyncio.TimeoutError:
            proc.kill()
            return ToolError(
                message="Command timed out",
                brief=f"Timeout after {params.timeout}s"
            )

        output = stdout.decode() + stderr.decode()

        return ToolOk(
            output=output,
            message=f"Exit code: {proc.returncode}"
        )
```

**ReadFile Tool:**

```python
# tools/file/read.py (109 lines)
class ReadFile(CallableTool2[Params]):
    """Read file contents"""

    class Params(BaseModel):
        path: str
        start_line: int | None = None
        end_line: int | None = None

    async def __call__(self, params: Params) -> ToolReturnType:
        file_path = self._runtime.work_dir / params.path

        if not file_path.exists():
            return ToolError(
                message=f"File not found: {params.path}",
                brief="Not found"
            )

        # Read file
        async with aiofiles.open(file_path) as f:
            lines = await f.readlines()

        # Apply line range
        if params.start_line or params.end_line:
            start = params.start_line or 1
            end = params.end_line or len(lines)
            lines = lines[start - 1:end]

        content = "".join(lines)

        return ToolOk(
            output=content,
            message=f"Read {len(lines)} lines from {params.path}"
        )
```

**Grep Tool (with ripgrep):**

```python
# tools/file/grep.py (287 lines)
class Grep(CallableTool2[Params]):
    """Search file contents using ripgrep"""

    class Params(BaseModel):
        pattern: str
        path: str = "."
        file_pattern: str | None = None
        context_lines: int = 0

    async def __call__(self, params: Params) -> ToolReturnType:
        # Ensure ripgrep binary
        rg_path = await self.ensure_ripgrep()

        # Build ripgrep command
        cmd = [
            str(rg_path),
            "--color=never",
            "--line-number",
            f"--context={params.context_lines}",
            params.pattern,
            params.path,
        ]

        if params.file_pattern:
            cmd.extend(["--glob", params.file_pattern])

        # Execute
        proc = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
            cwd=self._runtime.work_dir,
        )

        stdout, stderr = await proc.communicate()
        output = stdout.decode()

        if proc.returncode == 0:
            return ToolOk(
                output=output,
                message=f"Found matches in {params.path}"
            )
        elif proc.returncode == 1:
            # No matches
            return ToolOk(
                output="No matches found",
                message="No matches"
            )
        else:
            # Error
            return ToolError(
                message=stderr.decode(),
                brief="Ripgrep error"
            )

    async def ensure_ripgrep(self) -> Path:
        """Download ripgrep if not found"""

        # Check: ~/.kimi/bin/rg
        rg_path = Path.home() / ".kimi" / "bin" / "rg"
        if rg_path.exists():
            return rg_path

        # Check system PATH
        rg_system = shutil.which("rg")
        if rg_system:
            return Path(rg_system)

        # Download from CDN
        platform = get_platform()  # "macos-aarch64", "linux-x86_64", etc.
        url = f"https://cdn.example.com/ripgrep/{platform}/rg"

        async with aiohttp.ClientSession() as session:
            async with session.get(url) as resp:
                content = await resp.read()

        rg_path.parent.mkdir(parents=True, exist_ok=True)
        rg_path.write_bytes(content)
        rg_path.chmod(0o755)

        return rg_path
```

**Task Tool (Subagent Delegation):**

```python
# tools/task.py (87 lines)
class Task(CallableTool2[Params]):
    """Delegate to subagent"""

    class Params(BaseModel):
        subagent: str
        prompt: str

    async def __call__(self, params: Params) -> ToolReturnType:
        # Get subagent
        subagent = self._soul.subagents.get(params.subagent)
        if not subagent:
            return ToolError(
                message=f"Unknown subagent: {params.subagent}",
                brief="Not found"
            )

        # Create child context
        child_context = Context.create_child(self._context)
        child_context.append_message(Message(
            role="user",
            content=[TextPart(text=params.prompt)]
        ))

        # Run subagent
        subagent_soul = subagent.with_context(child_context)
        await subagent_soul.run(wire_send=self._wire_send)

        # Get result
        final_message = child_context.history[-1]
        result = final_message.content[0].text

        return ToolOk(
            output=result,
            message=f"Subagent '{params.subagent}' completed"
        )
```

### 7.3 MCP Integration

**Model Context Protocol Support:**

```python
# tools/mcp.py
from fastmcp import FastMCP

class MCPTool(CallableTool2):
    """Wrapper for MCP tool"""

    def __init__(self, mcp_tool: fastmcp.Tool, client: fastmcp.Client):
        self.name = mcp_tool.name
        self.description = mcp_tool.description
        self.params = mcp_tool.params
        self._client = client

    async def __call__(self, params: dict) -> ToolReturnType:
        # Call MCP tool
        result = await self._client.call_tool(self.name, params)

        return ToolOk(
            output=result.output,
            message="MCP tool executed"
        )

# Load MCP servers
async def load_mcp_servers(config: Config) -> list[MCPTool]:
    """Load tools from MCP servers"""

    tools = []

    for server_name, server_config in config.mcp_servers.items():
        # Connect to MCP server
        client = fastmcp.Client(
            command=server_config.command,
            args=server_config.args,
            env=server_config.env,
        )

        async with client:
            # List available tools
            for tool in await client.list_tools():
                tools.append(MCPTool(tool, client))

    return tools
```

---

## 8. State Management

### 8.1 Configuration

**Config File:** `~/.kimi/config.json`

```python
# config.py (285 lines)
class LLMProvider(BaseModel):
    """LLM provider configuration"""
    type: Literal["kimi", "openai", "openai_responses", "anthropic"]
    base_url: str | None = None
    api_key: SecretStr

class LLMModel(BaseModel):
    """LLM model configuration"""
    provider: str              # References LLMProvider name
    model: str                # Model ID
    max_context_size: int     # Token limit
    capabilities: list[str]   # e.g., ["image_in", "tool_calling"]

class LoopControl(BaseModel):
    """Agent loop parameters"""
    max_agent_steps: int = 100
    compact_context_at: float = 0.9  # Compact when 90% full

class Config(BaseModel):
    """Root configuration"""
    default_model: str
    models: dict[str, LLMModel]
    providers: dict[str, LLMProvider]
    loop_control: LoopControl
    services: Services
    mcp_servers: dict[str, MCPServerConfig]
    auto_approval_rules: list[AutoApprovalRule]

# Loading
def load_config() -> Config:
    config_path = Path.home() / ".kimi" / "config.json"

    if not config_path.exists():
        # Run setup wizard
        run_setup_wizard()

    return Config.parse_file(config_path)
```

**Setup Wizard:**

```python
async def run_setup_wizard():
    """Interactive configuration wizard"""

    console.print("[bold]Kimi CLI Setup[/bold]\n")

    # 1. Choose provider
    provider_type = prompt_toolkit.prompt(
        "Provider (kimi/openai/anthropic): "
    )

    # 2. Enter API key
    api_key = prompt_toolkit.prompt(
        "API Key: ",
        is_password=True
    )

    # 3. Choose model
    model = prompt_toolkit.prompt(
        "Model ID: "
    )

    # 4. Create config
    config = Config(
        default_model="default",
        models={
            "default": LLMModel(
                provider="default",
                model=model,
                max_context_size=128000,
                capabilities=["tool_calling"]
            )
        },
        providers={
            "default": LLMProvider(
                type=provider_type,
                api_key=SecretStr(api_key)
            )
        },
        loop_control=LoopControl(),
        services=Services(),
        mcp_servers={},
        auto_approval_rules=[]
    )

    # 5. Save
    config_path = Path.home() / ".kimi" / "config.json"
    config_path.parent.mkdir(parents=True, exist_ok=True)
    config_path.write_text(config.json(indent=2))
```

### 8.2 Persistence Strategy

**File Locations:**

```
~/.kimi/
â”œâ”€â”€ config.json                  # Configuration
â”œâ”€â”€ metadata.json                # Working directory metadata
â”œâ”€â”€ logs/                        # Application logs
â”‚   â””â”€â”€ kimi.log (rotated daily)
â”œâ”€â”€ user-history/                # Input history (per working dir)
â”‚   â””â”€â”€ <md5-of-cwd>.jsonl
â”œâ”€â”€ bin/                         # Downloaded binaries
â”‚   â””â”€â”€ rg (ripgrep)
â””â”€â”€ sessions/                    # Conversation histories
    â””â”€â”€ <work-dir-path>/
        â”œâ”€â”€ <session-id-1>.jsonl
        â”œâ”€â”€ <session-id-2>.jsonl
        â””â”€â”€ ...
```

**JSONL Format for History:**

```jsonl
{"role":"user","content":[{"type":"text","text":"Write hello world"}]}
{"role":"_checkpoint","id":0,"timestamp":1730000000.0}
{"role":"assistant","content":[{"type":"text","text":"I'll write hello world"},{"type":"tool_call","name":"WriteFile","input":{"path":"hello.py","content":"print('hello world')"}}]}
{"role":"user","content":[{"type":"tool_result","tool_name":"WriteFile","output":"File written"}]}
{"role":"_usage","token_count":1234}
{"role":"_checkpoint","id":1,"timestamp":1730000010.0}
```

**Benefits:**
- Append-only (crash-safe)
- Human-readable
- Easy to debug
- Supports checkpoints
- Token tracking

---

## 9. Design Patterns

### 9.1 Event-Driven UI Updates

**Wire Protocol** (`ui/wire/message.py`):

```python
# Event types
@dataclass
class StepBegin:
    step_no: int

@dataclass
class MessagePart:
    text: str

@dataclass
class ToolCallBegin:
    tool_name: str
    tool_input: dict

@dataclass
class ToolCallOutput:
    tool_name: str
    output: str

@dataclass
class StatusUpdate:
    status: StatusSnapshot

@dataclass
class RunFinish:
    reason: str

# Wire send function
async def wire_send(event: WireEvent):
    """Send event to all subscribers"""

    # Serialize event
    json_str = json.dumps(asdict(event))

    # Write to stdout (for wire mode)
    print(json_str, flush=True)

    # Update live view (for shell mode)
    if live_view:
        live_view.handle_event(event)
```

**Usage in Agent:**

```python
async def run_agent(wire_send: Callable):
    wire_send(StepBegin(step_no=1))

    # Streaming LLM response
    async for chunk in llm_stream:
        wire_send(MessagePart(text=chunk.text))

    # Tool execution
    wire_send(ToolCallBegin(
        tool_name="Bash",
        tool_input={"command": "ls"}
    ))

    result = await execute_tool("Bash", {"command": "ls"})

    wire_send(ToolCallOutput(
        tool_name="Bash",
        output=result.output
    ))

    wire_send(RunFinish(reason="stop"))
```

### 9.2 Dependency Injection

**Tool Dependencies:**

```python
# Tools declare dependencies in __init__
class BashTool:
    def __init__(
        self,
        approval: Approval,      # Injected
        runtime: Runtime,        # Injected
        config: Config,          # Injected
        **kwargs
    ):
        self._approval = approval
        self._runtime = runtime
        self._config = config

# Agent loader injects dependencies
tool_deps = {
    Approval: approval_instance,
    Runtime: runtime_instance,
    Config: config_instance,
}

def instantiate_tool(tool_class: type) -> Tool:
    # Get required dependencies from __init__ signature
    sig = inspect.signature(tool_class.__init__)

    kwargs = {}
    for param_name, param in sig.parameters.items():
        if param_name == "self":
            continue

        param_type = param.annotation
        if param_type in tool_deps:
            kwargs[param_name] = tool_deps[param_type]

    return tool_class(**kwargs)
```

### 9.3 Builder Pattern

```python
# tools/utils.py
class ToolResultBuilder:
    """Fluent API for building tool results"""

    def __init__(self):
        self._output: list[str] = []
        self._metadata: dict = {}

    def write(self, text: str) -> ToolResultBuilder:
        """Append to output"""
        self._output.append(text)
        return self

    def write_line(self, text: str) -> ToolResultBuilder:
        """Append line to output"""
        self._output.append(text + "\n")
        return self

    def set_metadata(self, key: str, value: Any) -> ToolResultBuilder:
        """Set metadata"""
        self._metadata[key] = value
        return self

    def ok(self, message: str) -> ToolOk:
        """Build successful result"""
        return ToolOk(
            output="".join(self._output),
            message=message,
            metadata=self._metadata
        )

    def error(self, message: str, brief: str) -> ToolError:
        """Build error result"""
        return ToolError(
            message=message,
            brief=brief,
            metadata=self._metadata
        )

# Usage
builder = ToolResultBuilder()
builder.write_line("Step 1: Done")
builder.write_line("Step 2: Done")
builder.set_metadata("steps", 2)
return builder.ok("All steps completed")
```

### 9.4 Async Context Managers

```python
# MCP client usage
async with fastmcp.Client(command="mcp-server") as client:
    tools = await client.list_tools()
    result = await client.call_tool("my_tool", params)

# File operations
async with aiofiles.open("file.txt") as f:
    content = await f.read()

# Live view
with Live(console=console) as live:
    for update in stream:
        live.update(render(update))
```

### 9.5 Retry with Tenacity

```python
from tenacity import (
    retry,
    retry_if_exception,
    wait_exponential_jitter,
    stop_after_attempt
)

@retry(
    retry=retry_if_exception(lambda e: isinstance(e, RetryableError)),
    wait=wait_exponential_jitter(initial=0.3, max=5, jitter=0.5),
    stop=stop_after_attempt(10),
    reraise=True,
)
async def llm_call():
    """Call LLM with automatic retry on transient errors"""
    return await kosong.step(...)

# Retries on:
# - Network errors (timeout, connection refused)
# - Rate limit errors (429)
# - Server errors (5xx)
#
# Exponential backoff:
# - Attempt 1: 0.3s
# - Attempt 2: 0.6s
# - Attempt 3: 1.2s
# - ...
# - Attempt 10: 5s (max)
```

---

## 10. Code Quality

### 10.1 Type Safety

**Score: 9/10** âœ…

**Full Type Hints:**

```python
# All functions have type hints
async def load_agent(
    spec_path: Path,
    config: Config,
    runtime: Runtime,
) -> KimiSoul:
    ...

# All classes use type hints
class Context:
    _history: list[Message]
    _token_count: int
    _next_checkpoint_id: int

# Pydantic models for validation
class Params(BaseModel):
    command: str
    timeout: int = 120

# Type checking with pyright (strict mode)
$ pyright
0 errors found
```

**Benefits:**
- Catch bugs at development time
- Better IDE autocomplete
- Self-documenting code
- Runtime validation (Pydantic)

### 10.2 Testing

**Score: 8/10** âœ…

**Test Structure:**

```
tests/
â”œâ”€â”€ test_agent_spec.py
â”œâ”€â”€ test_bash.py
â”œâ”€â”€ test_default_agent.py (539 LOC - largest test)
â”œâ”€â”€ test_glob.py (313 LOC)
â”œâ”€â”€ test_grep.py (364 LOC)
â”œâ”€â”€ test_load_agent.py
â”œâ”€â”€ test_*_file.py (read, write, str_replace, patch)
â”œâ”€â”€ test_task.py
â”œâ”€â”€ test_todo.py
â””â”€â”€ conftest.py (fixtures)
```

**Test Patterns:**

```python
import pytest
from unittest.mock import AsyncMock

@pytest.fixture
async def mock_llm():
    """Mock LLM for deterministic tests"""
    llm = AsyncMock()
    llm.step.return_value = StepResult(
        message=Message(
            role="assistant",
            content=[TextPart(text="Test response")]
        ),
        finish_reason="stop"
    )
    return llm

@pytest.fixture
async def agent(tmp_path, mock_llm):
    """Create test agent"""
    spec = create_agent_spec(tmp_path)
    return await load_agent(spec, mock_llm)

@pytest.mark.asyncio
async def test_bash_execution(agent):
    """Test bash tool execution"""
    result = await agent.tools["Bash"]({
        "command": "echo hello"
    })

    assert isinstance(result, ToolOk)
    assert "hello" in result.output
```

**Coverage:**
- Unit tests for all tools
- Integration tests for agent loop
- Async test support (pytest-asyncio)
- Mock LLM for deterministic behavior
- ~70% coverage (good)

### 10.3 Documentation

**Score: 9/10** âœ…

**Comprehensive Docs:**
- `README.md`: User guide
- `AGENTS.md`: Architecture documentation (excellent!)
- `CHANGELOG.md`: Version history
- `CONTRIBUTING.md`: Contributor guide
- Inline docstrings for all public APIs
- Tool descriptions in `.md` files

**Example Tool Doc:**

```markdown
<!-- tools/bash/tool.md -->

# Bash Tool

Execute a bash command in the working directory.

## Usage

```yaml
tool: Bash
params:
  command: "ls -la"
  timeout: 120
```

## Returns

- `stdout` and `stderr` combined
- Exit code in message

## Notes

- Interactive commands (vim, nano) will not work
- Commands timeout after specified seconds (default: 120)
- Requires user approval unless auto-approved
```

### 10.4 Code Organization

**Score: 9/10** âœ…

**Strengths:**
- Clear separation of concerns
- Modular tool system
- Event-driven architecture
- Async throughout
- Minimal dependencies

**Package Structure:**
```
kimi_cli/
â”œâ”€â”€ ui/           # UI implementations (isolated)
â”œâ”€â”€ soul/         # Agent logic (core)
â”œâ”€â”€ tools/        # Tool implementations (pluggable)
â”œâ”€â”€ config.py     # Configuration (single file)
â”œâ”€â”€ session.py    # Session management (single file)
â””â”€â”€ llm.py        # LLM factory (single file)
```

---

## 11. Key Learnings for Penguin

### 11.1 Python is Sufficient

**Evidence from Kimi-CLI:**
- âœ… 8,546 lines of production-ready Python
- âœ… No Go, Rust, or other compiled languages
- âœ… Excellent TUI with prompt-toolkit + rich
- âœ… Fast async I/O with asyncio + aiofiles
- âœ… Strong type safety with type hints + pyright
- âœ… Comprehensive testing with pytest
- âœ… Binary distribution with PyInstaller
- âœ… Cross-platform support

**Conclusion: Penguin should stick with Python!**

### 11.2 Recommended Libraries

**Core Stack:**
```python
# CLI/TUI
click                # CLI framework
prompt-toolkit       # Advanced terminal input
rich                 # Terminal formatting

# Async
asyncio              # Built-in async runtime
aiofiles             # Async file I/O
aiohttp              # Async HTTP client

# Validation
pydantic             # Data validation & settings

# Quality
pytest               # Testing
pytest-asyncio       # Async test support
pyright              # Type checking
ruff                 # Linting & formatting

# Utilities
tenacity             # Retry logic
watchfiles           # File watching
```

### 11.3 Architecture Patterns to Adopt

**1. Event-Driven UI Updates**

```python
# Wire protocol for decoupling agent from UI
@dataclass
class TokenUpdate:
    text: str

async def agent_loop(wire_send: Callable):
    for token in stream:
        wire_send(TokenUpdate(text=token))

# Multiple subscribers
wire_send = create_wire_send([
    ui_subscriber,
    log_subscriber,
    telemetry_subscriber
])
```

**2. JSONL Conversation History**

```python
# Append-only, crash-safe
async with aiofiles.open("history.jsonl", "a") as f:
    await f.write(json.dumps(message.dict()) + "\n")

# Benefits:
# - Crash-safe (append-only)
# - Human-readable
# - Easy to debug
# - Supports checkpoints
```

**3. Checkpoint System**

```python
# Time-travel debugging
await context.checkpoint()  # Save state
await context.revert_to(checkpoint_id)  # Rollback

# D-Mail: Send message to past
await send_dmail(checkpoint_id=2, message="Try different approach")
```

**4. Dependency Injection for Tools**

```python
# Tools declare dependencies
class MyTool:
    def __init__(self, approval: Approval, runtime: Runtime):
        ...

# Agent loader injects them
tool = instantiate_tool(MyTool, dependencies)
```

**5. YAML Agent Specs**

```yaml
version: 1
agent:
  name: "My Agent"
  system_prompt_path: "./system.md"
  tools:
    - module:Class
  subagents:
    research: ...
```

### 11.4 Tool System Improvements

**Pydantic Parameters:**

```python
class Params(BaseModel):
    path: str
    content: str
    encoding: str = "utf-8"

# Automatic validation
# JSON schema generation for LLM
# Type-safe access
```

**Markdown Descriptions:**

```python
# Load from separate .md file
description = load_desc(Path(__file__).parent / "tool.md")

# Benefits:
# - Better formatting
# - Easier to edit
# - Longer descriptions
```

**Structured Returns:**

```python
# Instead of strings, return structured results
return ToolOk(
    output="...",
    message="Success",
    metadata={"lines_changed": 10}
)

# Or:
return ToolError(
    message="File not found",
    brief="Not found"
)
```

### 11.5 UI/UX Patterns

**Dual-Mode Shell:**

```python
# Agent mode: Send to LLM
# Shell mode: Direct shell execution
# Toggle with Ctrl-X

if agent_mode:
    await agent.send_message(input)
else:
    await execute_shell_command(input)
```

**File Mention Completion:**

```python
# @-mention style completion
# Types "@src/ma" â†’ suggests "@src/main.py"
# Fuzzy matching
# Respects .gitignore
```

**Image Paste Support:**

```python
# Ctrl-V to paste image
# Converts to base64
# Sends as ImageURLPart
```

**Live Updating Display:**

```python
from rich.live import Live

with Live(console=console) as live:
    for update in stream:
        live.update(render(update))
```

### 11.6 Testing Strategy

**Mock LLM Calls:**

```python
@pytest.fixture
async def mock_llm():
    llm = AsyncMock()
    llm.step.return_value = StepResult(
        message=Message(role="assistant", content=[TextPart(text="Test")])
    )
    return llm
```

**Async Test Support:**

```python
@pytest.mark.asyncio
async def test_tool():
    result = await tool.execute(params)
    assert isinstance(result, ToolOk)
```

**Fixture-Based Dependencies:**

```python
@pytest.fixture
def runtime(tmp_path):
    return Runtime(work_dir=tmp_path)

@pytest.fixture
def approval():
    approval = Mock()
    approval.request = AsyncMock(return_value=True)
    return approval
```

### 11.7 Distribution

**PyInstaller for Binaries:**

```python
# pyinstaller.spec
a = Analysis(
    ['kimi_cli/__main__.py'],
    pathex=[],
    binaries=[],
    datas=[
        ('kimi_cli/agents', 'agents'),
    ],
    hiddenimports=[],
    hookspath=[],
)

pyz = PYZ(a.pure, a.zipped_data)

exe = EXE(
    pyz,
    a.scripts,
    a.binaries,
    a.zipfiles,
    a.datas,
    name='kimi',
    console=True,
)
```

**Benefits:**
- Standalone binary
- No Python installation required
- Cross-platform
- ~20MB binary size

---

## 12. Python vs TypeScript CLI Decision

### 12.1 Evidence for Python

**Kimi-CLI proves Python is excellent for CLI:**
- âœ… 8,546 lines of production-ready code
- âœ… Sophisticated TUI (prompt-toolkit + rich)
- âœ… Fast async I/O (no performance issues)
- âœ… Strong type safety (type hints + pyright)
- âœ… Comprehensive testing (pytest)
- âœ… Binary distribution (PyInstaller)
- âœ… Cross-platform (macOS, Linux, Windows)

**No downsides compared to TypeScript/Node:**
- Performance: Async I/O is sufficient
- Type safety: Type hints + pyright â‰ˆ TypeScript
- Ecosystem: Better for AI/ML tools
- Development speed: Faster iteration

### 12.2 Python Advantages for Penguin

**1. Code Reuse**
- Share code between backend and CLI
- Same data models (Pydantic)
- Same tool implementations
- No language barrier

**2. Ecosystem**
- Rich AI/ML libraries (transformers, langchain, etc.)
- Excellent terminal libraries (prompt-toolkit, rich)
- Comprehensive standard library
- Large community

**3. Development Speed**
- Faster iteration
- Easier debugging
- Better introspection
- Dynamic typing when needed

**4. Integration**
- Direct access to Penguin engine
- No gRPC needed (unless desired)
- Simpler deployment
- Fewer dependencies

### 12.3 TypeScript Advantages (Why Not Penguin?)

**Startup Time:**
- Node.js: ~50ms
- Python: ~100-200ms
- **But:** PyInstaller binaries are fast

**Memory Footprint:**
- Node.js: ~30MB
- Python: ~50MB
- **But:** Not significant for CLI

**Static Binary:**
- Node.js: pkg, nexe
- Python: PyInstaller
- **But:** Both work well

**Type Safety:**
- TypeScript: Built-in
- Python: Type hints + pyright
- **But:** Comparable

**Verdict:** No compelling reason to use TypeScript for Penguin CLI.

### 12.4 Recommendation: Pure Python

**For Penguin:**
1. âœ… **Stick with Python** for CLI
2. âœ… Use prompt-toolkit for TUI
3. âœ… Use rich for formatting
4. âœ… Use async/await throughout
5. âœ… Use pyright for type checking
6. âœ… Use PyInstaller for distribution

**Don't switch to:**
- âŒ TypeScript/Node.js (no compelling reason)
- âŒ Go (Python is sufficient)
- âŒ Rust (overkill for CLI)

---

## Conclusion

### Key Findings

1. **Pure Python is Proven**
   - Kimi-CLI demonstrates Python is excellent for sophisticated CLIs
   - 8,546 lines, production-ready, no performance issues
   - No Go, Rust, or other languages needed

2. **Modern Python is Fast**
   - Async I/O handles concurrency well
   - prompt-toolkit + rich = excellent TUI
   - PyInstaller creates fast binaries

3. **Strong Type Safety Achievable**
   - Type hints throughout
   - pyright in strict mode
   - Pydantic for runtime validation
   - Comparable to TypeScript

4. **Excellent Architecture Patterns**
   - Event-driven UI updates
   - JSONL conversation history
   - Checkpoint system
   - Dependency injection
   - YAML agent specs

### Recommendations for Penguin

**High Priority (Week 1-2):**
1. âœ… Stick with Python for CLI
2. Adopt prompt-toolkit for TUI
3. Adopt rich for formatting
4. Implement event-driven wire protocol
5. Use JSONL for conversation history

**Medium Priority (Week 3-4):**
1. Implement checkpoint system
2. Add approval flow for dangerous operations
3. YAML agent specifications
4. Dependency injection for tools
5. PyInstaller distribution

**Low Priority (Month 2+):**
1. Time-travel debugging (D-Mail)
2. Context compaction
3. MCP integration
4. Image paste support

### Final Assessment

**Grade: A** (Production-ready, excellent patterns)

Kimi-CLI is an outstanding reference for Penguin. It proves that:
- **Python is sufficient** for sophisticated CLIs
- **No TypeScript/Go needed** - Python handles everything
- **Modern patterns work** - async, type hints, pyright
- **Distribution is easy** - PyInstaller creates binaries

**Key Takeaway:** Penguin should continue with Python and adopt Kimi's proven patterns. The question isn't "Can Python do this?" - Kimi proves it can. The question is "Which patterns should we adopt?" - and the answer is: most of them!

---

**Next Steps:**
1. Review this analysis with team
2. Prioritize pattern adoption
3. Start with event-driven UI (wire protocol)
4. Implement JSONL conversation history
5. Add prompt-toolkit + rich for better TUI
6. Plan PyInstaller distribution
