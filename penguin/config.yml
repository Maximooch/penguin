# Check LiteLLM documentation to see how to use other providers
# https://docs.litellm.ai/docs/providers

model:
  default: "anthropic/claude-3-5-sonnet-20240620"
  provider: "openrouter"
  client_preference: "openrouter"  # Use 'litellm' for LiteLLM gateway, 'native' for direct SDK, or 'openrouter' for OpenRouter
  use_assistants_api: false     # Toggle OpenAI Assistants API usage
  streaming_enabled: true       # Enable streaming support
  max_tokens: 64000
  # temperature: 0.4
  context_window: 200000
  # temperature: 0.4


api:
  base_url: null # LiteLLM uses api_base per model, remove global one? Or keep for default? Let's remove for clarity.
  # If you're using ollama via LiteLLM, set api_base in its model_config below

# base_url: "https://api.anthropic.com/v1/messages"
#   # If you're using ollama, set the base_url to the ollama server url, which is usually http://127.0.0.1:11434
#   # It'll tell you if you do `ollama serve` in the terminal.

# /Users/maximusputnam/Documents/code/Penguin/workspace

# Workspace Configuration
workspace:
  path: "/Users/maximusputnam/Documents/code/Penguin/penguin_workspace"  # Default, can be overridden
  create_dirs:  # Subdirectories to create
    - conversations
    - memory_db
    - logs
    - notes
    - projects
    - context


diagnostics:
  enabled: false  # Global switch for diagnostics
  max_context_tokens: 200000
  log_to_file: false
  log_path: "logs/penguin.log"  # Optional, only used if log_to_file is true


# ------------------------------------------------------------------------------------------------
# Model Configs. You can ignore these unless you want to change the default model configs. 
# ------------------------------------------------------------------------------------------------
model_configs:
  # Example using LiteLLM for Anthropic (overrides default)
  anthropic/claude-3-5-sonnet-20240620:
    provider: anthropic
    client_preference: litellm # Explicitly use LiteLLM
    max_tokens: 8192
    temperature: 0.4
    vision_enabled: true # Explicitly enable vision

  # Example using Native Adapter for Anthropic (different preference)
  anthropic-native/claude-3-opus-20240229: # Use a distinct name if needed
    model: claude-3-opus-20240229 # Simple name for native adapter
    provider: anthropic
    client_preference: native # Explicitly use Native Adapter
    max_tokens: 4096
    temperature: 0.7
    vision_enabled: true

  # Example using LiteLLM for OpenAI
  openai/gpt-4o:
    provider: openai
    client_preference: litellm
    max_tokens: 4096
    temperature: 0.4
    vision_enabled: true
    
  # OpenRouter model examples
  openai/gpt-4o-openrouter:
    model: openai/gpt-4o
    provider: openrouter
    client_preference: openrouter
    max_tokens: 4096
    temperature: 0.7
    vision_enabled: true
    
  anthropic/claude-3-5-sonnet-openrouter:
    model: anthropic/claude-3-5-sonnet-20240620
    provider: openrouter
    client_preference: openrouter
    max_tokens: 8192
    temperature: 0.7
    vision_enabled: true
    
  google/gemini-2.0-flash:
    model: google/gemini-2.0-flash-exp:free
    provider: openrouter
    client_preference: openrouter
    max_tokens: 4096
    temperature: 0.7
    vision_enabled: true

  # Example using LiteLLM for Ollama
  ollama/llama3:
    provider: ollama # Provider helps find API key env vars if needed, though often not for ollama
    client_preference: litellm
    api_base: "http://localhost:11434" # API base needed for Ollama
    max_tokens: 4096
    temperature: 0.7
    vision_enabled: false

  # Example old style config (will default to client_preference: native)
  gpt-4: # Model name used directly if native client is chosen
    provider: openai
    # client_preference: native # (Implied default)
    max_tokens: 8192
    temperature: 0.7

  # Free models for testing and fallback
  openai/gpt-3.5-turbo-free:
    model: openai/gpt-3.5-turbo-0125
    provider: openrouter
    client_preference: openrouter
    max_tokens: 4096
    temperature: 0.7
    vision_enabled: false
    
  google/gemini-pro-free:
    model: google/gemini-pro
    provider: openrouter
    client_preference: openrouter
    max_tokens: 4096
    temperature: 0.7
    vision_enabled: false
    
  mistral/mistral-large-free:
    model: mistral/mistral-large-latest
    provider: openrouter
    client_preference: openrouter
    max_tokens: 4096
    temperature: 0.7
    vision_enabled: false

  # DeepSeek model with adjusted parameters to reduce empty responses
  deepseek/deepseek-chat:
    model: deepseek/deepseek-chat-v3-0324:free
    provider: openrouter
    client_preference: openrouter
    max_tokens: 8192
    temperature: 0.9  # Higher temperature to avoid default outputs
    top_p: 0.95  # Slightly reduce top_p to encourage more varied responses
    frequency_penalty: 0.1  # Add a small frequency penalty
    vision_enabled: false

# Add other non-sensitive configuration options as needed

# Tools:
# Toggle whatever tools you want to use, config.py then makes a list sending to core.py to inform what tools are available.
# 3rd party tools need to have a info.yml (txt, json, whatever type of text file) linked to that particular tool. So it could be loaded as a mini prompt.


# https://api.openai.com/v1