# Penguin Python Library & SDK Roadmap

> Draft ‑ {{DATE}}

This document lays out the work needed to publish **Penguin** as a clean, consumable *Python package* (`penguin-ai`) **and** generate an optional Python SDK for the remote Web API.  We want a developer to be able to:

```python
pip install penguin-ai            # in-process core & engine
pip install penguin-ai[web]       # adds FastAPI server extras
pip install penguin-ai[sdk]       # generated client for remote HTTP API
```

---

## 0. Guiding Principles

1. Stable, minimal public surface — everything else is internal.
2. Lean default dependency set; optional extras for heavy features.
3. Semantic-versioning across **library**, **CLI**, **web extra**, **SDK**.
4. 1st-class type hints and autogenerated docs.

---

## 1. Public Library API Freeze (📋 Week 1-2)

1. **Define `__all__` exports** in `penguin/__init__.py`  
   – Likely: `PenguinCore`, `Engine`, `ToolManager`, `load_config`, `__version__`.
2. Create `docs/python_api_reference.md` derived from docstrings with `pdoc`.
3. Add `tests/library_public_surface_test.py` that `import *` from the root and asserts basic behaviour.
4. Tag commit `v0.2.0-alpha1` once passing.

---

## 2. Package Layout Cleanup (🗂  Week 2)

Task | Notes
---|---
Move FastAPI server code under `penguin.web.*` | Avoid polluting top-level namespace.
Move CLI entry-points under `penguin.cli.*` | Keep import graph thin.
Rename `penguin/tools/...` notes folders to `penguin/tools/_notes` | Exclude from wheel.
Ensure `setup.cfg` / `MANIFEST.in` exclude tests & notebooks | Reduce wheel size.

---

## 3. Extras Management (⚙️ Week 2-3)

```toml
[project.optional-dependencies]
web = ["fastapi>=0.110", "uvicorn[standard]>=0.29", "websockets>=12"]
memory = ["chromadb>=0.4.22", "lancedb>=0.5"]
llm_openai = ["openai>=1.30"]
llm_transformers = ["transformers>=4.40", "torch>=2"]
sdk = ["openapi-python-client>=0.20"]
```

Add guard clauses so that importing `penguin.web.server` raises a helpful error if `fastapi` is missing.

---

## 4. Versioning & Release Automation (🚀 Week 3)

1. Adopt `__version__` from `importlib.metadata.version(__name__)` in `penguin/__init__.py`.
2. Use **hatchling** or **pdm-backend**?  Stay on `setuptools.build_meta` for now.
3. GitHub Actions:
   - `test` – matrix `py{38,39,310,311}` minimal deps.
   - `test-web` – run web extra tests.
   - `build` – build sdist + wheel.
   - `publish` – on tag `v*` upload to TestPyPI, on `main` w/ proper secret upload to PyPI.
4. Changelog automation (`github-release-notes`, `towncrier`, or simple `scripts/update_changelog.py`).

---

## 5. Generated Web-API Python SDK (📡 Week 3-4)

1. Add OpenAPI generation step: `fastapi-codegen` or introspection route.
2. Use `openapi-python-client` to emit client into `penguin/sdk_client/`.
3. Publish as part of `penguin-ai[sdk]` extra; top-level import: `from penguin.sdk import Client`.
4. Provide `examples/sdk_chat.py` demo.
5. Add CI test that spins up local server and uses SDK to send a message.

---

## 6. Documentation & Examples (📚 Week 4)

• `docs/guides/library_quickstart.md` – in-process usage.  
• `docs/guides/sdk_quickstart.md` – remote API usage.  
• Autogenerated API docs via `mkdocs + mkdocstrings`.  
• Jupyter notebook examples in `examples/notebooks/` (smoke-tested by CI).

---

## 7. Acceptance Criteria

- `pip install penguin-ai` imports in <1 s on M-series Mac.
- `import penguin; penguin.PenguinCore.create()` works without extra deps.
- `pip install penguin-ai[web]` starts the FastAPI server.
- SDK can hit `GET /api/v1/system/info` and pass a simple chat round-trip.
- Unit-test coverage ≥ 80 % on library public surface.

---

## 8. Backlog / Nice-to-have

- Binary wheels for common architectures (musl & manylinux).
- `poetry` alternative lock-file for reproducible installs.
- Mypy strict mode in CI.
- Auto-gen stubs for `penguin.tools.*` so IDEs know available actions.
- Cookiecutter template for external tool plug-ins.

---

## 9. Implementation Sprint #1 (Agent Wrapper & Streaming)

Goal: deliver a usable `PenguinAgent` wrapper + builder, disk-backed checkpoints, and dual sync/async surface.

### Work items

| ID | Item | Owner | Notes |
|----|------|-------|-------|
| 9-1 | `penguin/agent/__init__.py` with `PenguinAgent` (sync wrapper) and `PenguinAgentAsync` (`create_async`) | backend | Thin façade over `PenguinCore.create()` |
| 9-2 | `agent.builder.AgentBuilder` | backend | Collects kwargs → `.build()` returns `PenguinAgent` |
| 9-3 | Disk checkpoint helper (`workspace/checkpoints/{session_id}/{checkpoint_id}.json`) | backend | Use existing ConversationManager checkpoint API |
| 9-4 | SSE endpoint `/api/v1/chat/stream-sse` | backend | Mirrors existing WebSocket stream |
| 9-5 | Dual entry-point tests `tests/agent_smoke_test.py` | backend |  a) sync `.chat()`  b) async `.chat()`  c) streaming generator |
| 9-6 | README quick-start update | docs | Show one-liner usage & builder chain |
| 9-7 | Extras declaration check in `pyproject.toml` | release | Verify `[web]` & `[sdk]` extras compile |

### Acceptance for Sprint #1

1. `pytest tests/agent_smoke_test.py` passes on `py{38,39,310,311}`.
2. `python -c "from penguin import PenguinAgent; print(PenguinAgent().chat('ping'))"` works with no extras.
3. `curl -N http://localhost:8000/api/v1/chat/stream-sse` streams chunks.
4. New README section rendered correctly.

After this sprint we tag `v0.2.0-alpha2` and move on to SDK generation (#5 in main roadmap).

---

## 10. Future Considerations  
(ideas parked here for post-v0.2 planning)

1. **Agent presets** – Configurable subclasses (`CodingAgent`, `ResearchAgent`, `ChatAgent`) that auto-enable domain-specific tools.
2. **Pluggable Memory Providers** – builder `with_memory("faiss")` selects optional dependency group.
3. **Conversation Streaming Middleware** – yield deltas as [`openai.ChatCompletionChunk`]-style objects for drop-in compatibility with OpenAI clients.
4. **Structured Output Helpers** – `agent.ask_json(schema, prompt)` that loops until model returns valid JSON.
5. **Tool Autodiscovery** – `entry_points("penguin.tools")` so third-party packages can register tools at install time.
6. **Async context-manager** – `async with PenguinAgentAsync() as agent:` shuts down tool resources gracefully.
7. **CLI scaffold** – `penguin quickstart --model gpt-4o` generates boilerplate code using the builder pattern.
8. **Telemetry hooks** – user-defined coroutine that receives timing / token metrics after each call.
9. **Batch mode** – `agent.batch_chat(list[str])` funnels prompts through Engine with concurrency.
10. **Typed dataclass responses** – replace opaque dicts with pydantic models for better IDE support.

---

End of plan. 